{"cells":[{"cell_type":"markdown","metadata":{"id":"Acl59yuYy_Jr"},"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":["\"\"\"\n","This code is a Python translation of the MATLAB implementation of BioFaceNet, originally provided by the authors of the paper:\n","S. Alhagry, R. Bahgat, A. Elsayed, \"BioFaceNet: Deep Convolutional Neural Network for Face Photo-Sketch Recognition,\" arXiv:1908.10578 [cs.CV], 2019.\n","https://arxiv.org/pdf/1908.10578\n","\n","Original MATLAB code repository: https://github.com/ssma502/BioFaces\n","\n","Translated by: Ameya Tilaye\n","Date: 9/22/2024\n","\"\"\"\n","\n","\"\"\"\n","CelebA dataset: \n","@inproceedings{conf/iccv/LiuLWT15,\n","  added-at = {2018-10-09T00:00:00.000+0200},\n","  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n","  biburl = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},\n","  booktitle = {ICCV},\n","  crossref = {conf/iccv/2015},\n","  ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},\n","  interhash = {3f735aaa11957e73914bbe2ca9d5e702},\n","  intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},\n","  isbn = {978-1-4673-8391-2},\n","  keywords = {dblp},\n","  pages = {3730-3738},\n","  publisher = {IEEE Computer Society},\n","  timestamp = {2018-10-11T11:43:28.000+0200},\n","  title = {Deep Learning Face Attributes in the Wild.},\n","  url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},\n","  year = 2015\n","}\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"8T310V-Ty8Gd"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\owner\\anaconda3\\envs\\venv\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n","  warn(\n"]},{"ename":"AttributeError","evalue":"partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n","File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\venv\\Lib\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\venv\\Lib\\site-packages\\torchvision\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optical_flow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stereo_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     CarlaStereo,\n\u001b[0;32m      4\u001b[0m     CREStereo,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     SintelStereo,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcaltech\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Caltech101, Caltech256\n","File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\venv\\Lib\\site-packages\\torchvision\\datasets\\_optical_flow.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _read_png_16\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _read_pfm, verify_str_arg\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionDataset\n\u001b[0;32m     17\u001b[0m T1 \u001b[38;5;241m=\u001b[39m Tuple[Image\u001b[38;5;241m.\u001b[39mImage, Image\u001b[38;5;241m.\u001b[39mImage, Optional[np\u001b[38;5;241m.\u001b[39mndarray], Optional[np\u001b[38;5;241m.\u001b[39mndarray]]\n","File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\venv\\Lib\\site-packages\\torchvision\\datasets\\utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlparse\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_zoo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n","File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\venv\\Lib\\site-packages\\requests\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     charset_normalizer_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\venv\\Lib\\site-packages\\charset_normalizer\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n","File \u001b[1;32mc:\\Users\\owner\\anaconda3\\envs\\venv\\Lib\\site-packages\\charset_normalizer\\api.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n","\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"]}],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1672531914241,"user":{"displayName":"AMEYA TILAYE","userId":"13804243370329609474"},"user_tz":480},"id":"xCrO9h5aYm6o","outputId":"56cce1f8-dc3f-4844-b4c4-6aff6edc8260"},"outputs":[],"source":["print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9787,"status":"ok","timestamp":1672532379398,"user":{"displayName":"AMEYA TILAYE","userId":"13804243370329609474"},"user_tz":480},"id":"wpfdri8JgT8r","outputId":"2cfc194a-988e-4a27-d550-79de65c4e712"},"outputs":[],"source":["import scipy.io as spio\n","from scipy.io import loadmat\n","import torch\n","import numpy as np\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"elapsed":213,"status":"error","timestamp":1672533674551,"user":{"displayName":"AMEYA TILAYE","userId":"13804243370329609474"},"user_tz":480},"id":"bK65nAfJVjzi","outputId":"ab652334-7496-4da5-96e1-5a65496f8c59"},"outputs":[],"source":["\n","#import celebmat here\n","import torchvision\n","\n","\n","from torchvision.datasets import ImageFolder\n","from torchvision import transforms\n","\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from skimage import io\n","class CelebDataset(Dataset):                #importing CelebA dataset\n","      def __init__(self,data_dir,partition_file_path,split,transform):\n","          self.partition_file = pd.read_csv(partition_file_path)\n","          self.data_dir = data_dir\n","          self.split = split\n","          self.transform = transform\n","      def __len__(self):\n","          self.partition_file_sub = self.partition_file[self.partition_file[\"partition\"].isin(self.split)]\n","          return len(self.partition_file_sub)\n","      def __getitem__(self,idx):\n","          img_name = os.path.join(self.data_dir,\n","                                  self.partition_file_sub.iloc[idx, 0])\n","          image = io.imread(img_name)\n","          if self.transform:\n","              image = self.transform(image)\n","          return image\n","\n","batch_size = 50000\n","\n","transform = transforms.Compose(       #batch transformation\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","\n","#/content/drive/MyDrive/Visual Machines Group/CelebA\n","#celeba_data = torchvision.datasets.CelebA('/content/drive/MyDrive/Visual Machines Group/CelebA/Img/img_align_celeba.zip')\n","#celeba_data = torchvision.datasets.CelebA('/content/drive/MyDrive/Visual Machines Group/CelebA')\n","#download CelebA dataset\n","\n","data_loader = torch.utils.data.DataLoader(celeba_data,batch_size=4,shuffle=True,num_workers = *args.nThreads) #change batch size once you get this to work\n","\n","'''\n","'''\n","torchvision.datasets\n","\n","imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n","data_loader = torch.utils.data.DataLoader(imagenet_data,\n","                                          batch_size=4,\n","                                          shuffle=True,\n","                                          num_workers=args.nThreads)\n","\n","celeba:\n","\n","torchvision.datasets.CelebA(root: str, split: str = 'train', target_type: Union[List[str], str] = 'attr', transform: Optional[Callable] = None,target_transform: Optional[Callable] = None, download: bool = False)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJdzaz9ZJ_7I"},"outputs":[],"source":["spio.loadmat('/content/drive/MyDrive/Visual Machines Group/Newskincolour.mat') #'imdb/celebaimdb.mat'\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"elapsed":354,"status":"error","timestamp":1671324749152,"user":{"displayName":"AMEYA TILAYE","userId":"13804243370329609474"},"user_tz":480},"id":"vFZoyXqQn2mK","outputId":"b5632eb7-0d1e-4ed8-8f59-c85dbf37a7be"},"outputs":[],"source":["spio.loadmat('/content/drive/MyDrive/Visual Machines Group/illF.mat');\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ynfmcq2fnrdY"},"outputs":[],"source":["spio.loadmat('/content/drive/MyDrive/Visual Machines Group/Tmatrix.mat')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7E7YVDznkZv"},"outputs":[],"source":["illF = spio.loadmat('/content/drive/MyDrive/Visual Machines Group/illF.mat')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-r4YYsjnd2y"},"outputs":[],"source":["illumA = spio.loadmat('/content/drive/MyDrive/Visual Machines Group/illumA.mat')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOnvAmj9nO-r"},"outputs":[],"source":["spio.loadmat('/content/drive/MyDrive/Visual Machines Group/illumDmeasured.mat')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXiY-aS0nGxs"},"outputs":[],"source":["cmf = spio.loadmat('/content/drive/MyDrive/Visual Machines Group/rgbCMF.mat')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0zDsjhDm8jn"},"outputs":[],"source":["spio.loadmat('/content/drive/MyDrive/Visual Machines Group/XYZspace (1).mat')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHf5mdNhzASj"},"outputs":[],"source":["\n","\n","'''\n","%This MATLAB script configures the necessary library, load and prepare the data used in the experimentserver = true;\n","if server\n"," run('../vl_setupnn');\n"," run('../setup_autonn');\n"," load('../imdb/celebaimdb.mat');\n"," cmf = load('../util/rgbCMF.mat');\n"," load ('../util/illF.mat');\n"," load ('../util/illumA.mat');\n"," load ('../util/illumDmeasured.mat');\n"," load ('../util/Newskincolour.mat');\n"," load ('../util/Tmatrix.mat');\n","\n"," nimages = 50765;\n"," batchSize= 64;\n","'''\n","nimages = 50765;\n","batchSize= 64;"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mVyelzNfraf"},"outputs":[],"source":["#CameraSensitivityPCA.m\n","#PCA Model for Camera Sensitivities\n","#function [mu,PC,EV]= CameraSensitivityPCA(cmf) returns array\n","cmf = spio.loadmat('/content/drive/MyDrive/Visual Machines Group/rgbCMF.mat')\n","\n","def diagonalize(A): #Diagonalized matrix D = E^-1 * A * E\n","  #A = np.sqrt(EV[1:2])\n","  E = np.linalg.eig(A)\n","  E_inv = np.linalg.inv(A)\n","  X = np.dot(E_inv,A)\n","  D = np.dot(X,E)\n","  return D\n","\n","from sklearn.decomposition import PCA\n","def CameraSensitivityPCA(cmf):\n","  X = np.zeros((99,28))\n","  Y = np.zeros((99,28))\n","  redS = cmf.rgbCMF[1][1] #not sure if I need the cmf.\n","  greenS = cmf.rgbCMF[1][2]\n","  blueS = cmf.rgbCMF[1][3]\n","  for i in range(1,28):\n","    a = redS[:,i]\n","    b = np.sum(redS[:,i])\n","    Y[1:33,i]= np.linalg.lstsq(a.T, b.T)[0].T #np.true_divide(redS[:,i],sum(redS[:,i])) #true right array division\n","    c = greenS[:,i]\n","    d = np.sum(greenS[:,i])\n","    Y[34:66,i]= np.linalg.lstsq(c.T, d.T)[0].T #np.true_divide(greenS[:,i]) #greenS[:,i]./sum(greenS(:,i))\n","    e = blueS[:,i]\n","    f = np.sum(blueS[:,i])\n","    Y[67:99,i]= np.linalg.lstsq(e.T, f.T)[0].T #blueS(:,i)./sum(blueS(:,i))\n","    break\n","  pca = PCA(Y.transpose())  #[PC,~,EV,~,explained,mu] = pca(Y'); #principal component coefficients of the matrix transpose of Y\n","  mu = np.float32(mu.transpose())\n","  EV = np.float32(EV[1:2])\n","  A = np.sqrt(EV[1:2])\n","  PC = np.float32(PC[:,1:2] * diagonalize(A)) #PC = np.float32(PC[:,1:2] * diagonalized sqrt(EV[1:2]))\n","  return [mu,PC,EV]\n","''' On Lines 24,27,30:  c = b.' / a computes the solution of the equation c b = aT for c. Numpy can't do this directly, so solve bT cT = a for cT,transpose\n","the result:c = numpy.linalg.lstsq(a.T, b.T)[0].T\n","https://stackoverflow.com/questions/1001634/array-division-translating-from-matlab-to-python\n","'''\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoT64clEYbh-"},"outputs":[],"source":["#setup.m continued\n","illF = spio.loadmat('/content/drive/MyDrive/Visual Machines Group/illF.mat')\n","\n","\n","#predicted vectors' sizes\n","LightVectorSize = np.float32(15)\n","wavelength = np.float32(33)\n","bSize = np.float32(2)\n","\n","#normalize illuminations\n","\n","'''\n","resize before shaping   https://stackoverflow.com/questions/60028853/cannot-reshape-array-of-size-into-shape\n","datas= np.array([data], order='C')\n","datas.resize((1,28,28))\n","datas.shape\n","\n","\n","'''\n","'''\n","illF.resize((1,1,22,12))\n","illF.shape\n","'''\n","#np.reshape(illF,(1,1,33,12))\n","illumDmeasured = (illumDmeasured.transpose())\n","illumDmeasured = np.reshape(illumDmeasured,(1,1,33,22))\n","\n","#A (what is A?)\n","\n","#illumA = np.float32(illumA./np.sum(illumA[:])) look at ./ and transposes from the lstq part above\n","illumA =  np.float32(np.linalg.lstsq(illumA.T, np.sum(illumA[:]).T)[0].T)\n","'''\n","e = blueS[:,i]\n","    f = np.sum(blueS[:,i])\n","    Y[67:99,i]= np.linalg.lstsq(e.T, f.T)[0].T #blueS(:,i)./sum(blueS(:,i))\n","\n","'''\n","\n","#D (what is D?)\n","illumDNorm = np.float32(np.zeros(1,1,33,22))\n","for i in range(1,22):\n","  a = illumDmeasured[1,1,:,i]\n","  b = np.sum(illumDmeasured[1,1,:,i])\n","  illumDNorm[1,1,:,i] = np.linalg.lstsq(a.T, b.T)[0].T\n","  break\n","\n","'''\n","for i=1:22\n","    illumDNorm(1,1,:,i) = illumDmeasured(1,1,:,i)./sum(illumDmeasured(1,1,:,i));\n","    break\n","'''\n","#F(what is F?)\n","illumFNorm = np.float32(np.zeros(1,1,33,12))\n","'''\n","for i=1:12\n","    illumFNorm(1,1,:,i) = illF(1,1,:,i)./sum(illF(1,1,:,i));\n","end\n","'''\n","'''\n","% PCA model for camera sensitivities\n","[mu,PC,EVpca] = CameraSensitivityPCA(cmf);\n","%% predicted vectors sizes\n","LightVectorSize = single(15);  % 15 paramters of light model\n","wavelength = single(33);\n","bSize = single(2);  % 2 parameters of camera model\n","%% normalise illuminations\n","\n","illF           = reshape(illF,[1 1 33 12]);\n","illumDmeasured = illumDmeasured';\n","illumDmeasured = reshape(illumDmeasured, [1 1 33 22]);\n","% A\n","illumA         = single(illumA./sum(illumA(:)));\n","% D\n","illumDNorm = single(zeros(1,1, 33,22));\n","for i=1:22\n","    illumDNorm(1,1,:,i) = illumDmeasured(1,1,:,i)./sum(illumDmeasured(1,1,:,i));\n","end\n","% F\n","illumFNorm = single(zeros(1,1,33,12));\n","for i=1:12\n","    illumFNorm(1,1,:,i) = illF(1,1,:,i)./sum(illF(1,1,:,i));\n","end\n","%%\n","mu = Param('value',mu,'learningRate',0);\n","PC = Param('value',PC,'learningRate',0);\n","mu = gpuArray(mu);\n","PC = gpuArray(PC);\n","illumA = Param('value',illumA,'learningRate',0);\n","illumA = gpuArray(illumA);\n","illumFNorm = Param('value',illumFNorm,'learningRate',0);\n","illumFNorm = gpuArray(illumFNorm);\n","\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6bZgjKK39O4"},"outputs":[],"source":["#findT.m\n","\n","def findT(Tmatrix,BGrid): #BGrid defined in scalingNet.m\n","  Tmatrix = spio.loadmat('/content/drive/MyDrive/Visual Machines Group/Tmatrix.mat')\n","  T_RAW2XYZ = nn.Bilinear(Tmatrix,BGrid)\n","  #T_RAW2XYZ = vl_nnbilinearsampler(Tmatrix,BGrid);\n","  #%T_RAW2RGB.name ='T_RAW2RGB';\n","\n","\n","'''\n","function [T_RAW2XYZ] = findT(Tmatrix,BGrid)\n","% Inputs:\n","%     Tmatrix          : 128 x 128 x 9\n","%     BGrid            : 2 x B\n","%  Output:\n","%     T_RAW2RGB        : 1 x 1 x 9 x B\n","\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrQ06DTroERv"},"outputs":[],"source":["def multipleDecoders(x,y,nfilters,doubleconv):\n","  '''\n","  %FCN Build an autonn fully convolutional network,image to image tasks, skip connections from each encoder layer to corresponding\n","% decoder layer\n","%   Inputs kept the same:\n","%       nfilters - vector of filter sizes, 1:length-1 are used in the\n","%       contractive part of the network, the final value is the number of\n","%       filters applied at the lowest resolution before upsampling begins\n","%       x - input feature map as autonn layer\n","%       nclass - number of channels in output feature map\n","%\n","%\n","%   Outputs kept the same:\n","%       x - image to image prediction\n","%       y - output of lowest resolution - can be fed into fully connected\n","%       layers\n","\n","  '''\n","#nfilters = [length for length in range (1,length-1)]\n","nfilters = [32,32,64,96,128,256] #don't know how to read in array in Python\n","nlayers = np.array(np.size(nfilters)) #nlayers is a vector\n","for i in range(1, nlayers-1):\n","    if i == 1:\n","        #x = nn.Conv1d(x,[3 3 3 nfilters(i)], np.pad()) #figure out padding\n","        x = nn.Conv1d(i,np.size(nlayers),np.pad([3, 3, 3, nfilters[i]]), stride = 1)\n","    else:\n","        #x = nn.Conv1d(x, np.pad([3 3 nfilters(i-1) nfilters(i)]))\n","        x = nn.Conv1d(i,np.size(nlayers), np.pad([3, 3, nfilters[i-1], nfilters[i]]), stride = 1)\n","    break\n","    x = nn.BatchNorm1d(x)\n","    if doubleconv:#boolean\n","      x = nn.ReLU(x)\n","      #x = nn.Conv1d(x,[3 3 nfilters(i) nfilters(i)] , np.pad())\n","      x = nn.Conv1d(x,np.pad([3, 3, nfilters[i], nfilters[i]]), stride = 1)\n","      x = nn.BatchNorm1d(x) #batch normalization\n","      x = nn.ReLU(x)\n","      #x = nn.Conv1d(x,[3 3 nfilters(i) nfilters(i)] , np.pad())\n","      x = nn.Conv1d(x,np.pad([3, 3, nfilters[i], nfilters[i]]), stride = 1)\n","      x = nn.BatchNorm1d(x)\n","    break\n","    x_skip[i] = nn.ReLu(x) #x_skip{i} = vl_nnrelu(x);\n","    x = nn.MaxPool1d(x_skip[i],2,stride = 2) #x = vl_nnpool(x_skip{i}, 2, 'stride', 2) ;\n","break\n","\n","x = nn.Conv1d(x,np.pad([3, 3, nfilters[nlayers-1], nfilters[nlayers]]), stride = 1)\n","x = nn.BatchNorm1d(x) #batch normalization\n","y = nn.ReLU(x)\n","if doubleconv:\n","   x = nn.Conv1d(y,np.pad([3, 3, nfilters[nlayers], nfilters[nlayers]]), stride = 1)\n","   x = nn.BatchNorm1d(x) #batch normalization\n","   y = nn.ReLU(x)\n","   x = nn.Conv1d(y,np.pad([3, 3, nfilters[nlayers], nfilters[nlayers]]), stride = 1)\n","   x = nn.BatchNorm1d(x) #batch normalization\n","   y = nn.ReLU(x)\n","break\n","\n","\n","#Decoders\n","#vl_nnupsample = Layer.fromFunction(@autonn_upSample);\n","\n","for c in range(1, nclass):\n","  for i in range(nlayers-1,-1,1):\n","    if i == nlayers-1:\n","    #m = nn.Upsample(scale_factor=2, mode='nearest')\n","      x = nn.upSample(y)\n","    else:\n","      x = nn.upSample(x)\n","      break\n","    #x = cat(3,x,x_skip{i});\n","      x = nn.Conv1d(x,np.pad([3, 3, nfilters[i]+nfilters[i+1], nfilters[i]]), stride = 1)\n","      x = nn.BatchNorm1d(x) #batch normalization\n","      x = nn.ReLU(x)\n","\n","    if doubleconv:\n","      x = nn.Conv1d(x,np.pad([3, 3, nfilters[i], nfilters[i]]), stride = 1)\n","      x = nn.BatchNorm1d(x) #batch normalization\n","      x = nn.ReLU(x)\n","\n","      x = nn.Conv1d(x,np.pad([3, 3, nfilters[i], nfilters[i]]), stride = 1)\n","      x = nn.BatchNorm1d(x) #batch normalization\n","      x = nn.ReLU(x)\n","    break\n","  break\n","\n","  #Final Predictions\n","\n","  x = nn.Conv1d(x,np.pad([3, 3, nfilters[1], 1]), stride = 1)\n","  if c==1:\n","    z = x\n","  else:\n","    z = np.concatenate((z,x), axis = 3) # z = cat(3,z,x);\n","  break\n","break\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPNnS8DUT4bIYx6w9BOXiIh","mount_file_id":"1K-qEtIDl6fzyumVhDbxgw_IfvAJN132q","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
